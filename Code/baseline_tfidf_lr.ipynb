{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OBJ1 数据准备 + OBJ2 Baseline（TF‑IDF + Logistic Regression）\n",
        "\n",
        "本 notebook 面向 Sentiment140（training.1600000.processed.noemoticon.csv），完成：\n",
        "\n",
        "- **OBJ1**：清洗/预处理 + **60/10/30**（train/val/test）分层划分\n",
        "- **OBJ2**：建立 **baseline supervised sentiment classifier（TF‑IDF + Logistic Regression）**\n",
        "\n",
        "> 备注：保留否定词、感叹号/问号、重复字母、hashtag 等情感信号；避免过度清洗导致信息损失。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip -q install scikit-learn pandas numpy joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 读取数据（Sentiment140）\n",
        "\n",
        "原始字段：`target, ids, date, flag, user, text`。标准 Sentiment140 通常只有 **0(neg)** 与 **4(pos)**。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "633f78c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = Path(\"Dataset/training.1600000.processed.noemoticon.csv\")\n",
        "\n",
        "COLS = [\"target\",\"ids\",\"date\",\"flag\",\"user\",\"text\"]\n",
        "\n",
        "df = pd.read_csv(\n",
        "    DATA_PATH,\n",
        "    encoding=\"ISO-8859-1\",\n",
        "    header=None,\n",
        "    names=COLS,\n",
        "    usecols=[\"target\",\"text\"],  # baseline 只用标签与文本\n",
        "    dtype={\"target\":\"int32\",\"text\":\"string\"},\n",
        ")\n",
        "\n",
        "# 只保留 0/4（二分类）\n",
        "df = df[df[\"target\"].isin([0,4])].copy()\n",
        "df[\"label\"] = (df[\"target\"] == 4).astype(\"int8\")  # 1=pos, 0=neg\n",
        "\n",
        "print(df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 轻量但“情感友好”的预处理\n",
        "\n",
        "原则：\n",
        "- ✅ 保留否定信息：**不删除 stopwords**（或至少保留 not/no/never/n't）\n",
        "- ✅ 保留强度信号：`! ?`、重复字符（soooo）、全部大写的强调\n",
        "- ✅ 保留 hashtag 语义（#happy）\n",
        "- ✅ 只移除低价值噪声：URL、@提及、HTML、冗余空白\n",
        "\n",
        "> 这里采用“最小损失清洗”，把复杂特征交给 TF‑IDF（含 bigram）学习。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
        "USER_RE = re.compile(r\"@\\w+\")\n",
        "HTML_RE = re.compile(r\"&\\w+;\")\n",
        "# 仅移除除常见情感符号外的杂字符：保留 ! ? # '（用于n't）\n",
        "BAD_CHARS_RE = re.compile(r\"[^0-9A-Za-z\\s!?#!'’]\")\n",
        "\n",
        "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
        "REPEAT_RE = re.compile(r\"(.)\\1{3,}\")  # 超过3次重复归一\n",
        "\n",
        "def normalize_repeats(text: str) -> str:\n",
        "    # soooo -> soo（保留一定强度信息）\n",
        "    return REPEAT_RE.sub(r\"\\1\\1\", text)\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    t = str(text)\n",
        "    t = HTML_RE.sub(\" \", t)\n",
        "    t = URL_RE.sub(\" <URL> \", t)\n",
        "    t = USER_RE.sub(\" <USER> \", t)\n",
        "    t = normalize_repeats(t)\n",
        "    # 保留 hashtag：#happy -> HASHTAG_happy（让 tfidf 能吃到）\n",
        "    t = re.sub(r\"#(\\w+)\", r\" HASHTAG_\\1 \", t)\n",
        "    # 统一引号，保留 n't\n",
        "    t = t.replace(\"’\", \"'\")\n",
        "    t = BAD_CHARS_RE.sub(\" \", t)\n",
        "    t = MULTISPACE_RE.sub(\" \", t).strip()\n",
        "    return t.lower()\n",
        "\n",
        "df[\"text_clean\"] = df[\"text\"].map(preprocess)\n",
        "\n",
        "df[[\"text\",\"text_clean\",\"label\"]].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 分层划分：60/10/30（train/val/test）\n",
        "\n",
        "按照 Interim Report 的 OBJ1 要求进行固定比例划分，并使用 `stratify` 保持正负样本比例一致。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# 先切出 train 60% 与 temp 40%\n",
        "train_df, temp_df = train_test_split(\n",
        "    df[[\"text_clean\",\"label\"]],\n",
        "    test_size=0.40,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=df[\"label\"],\n",
        ")\n",
        "\n",
        "# 再把 temp 40% 切成 val 10% 与 test 30%\n",
        "# val 在总量占 10% => 在 temp 中占 10/40 = 0.25\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.75,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=temp_df[\"label\"],\n",
        ")\n",
        "\n",
        "def dist(name, d):\n",
        "    return name, len(d), float(d[\"label\"].mean())\n",
        "\n",
        "for item in [dist(\"train\", train_df), dist(\"val\", val_df), dist(\"test\", test_df)]:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. OBJ2 Baseline：TF‑IDF + Logistic Regression\n",
        "\n",
        "推荐设置：\n",
        "- `ngram_range=(1,2)`：捕捉 **not good** 这类否定 bigram\n",
        "- `sublinear_tf=True`：减少超高频词的影响\n",
        "- `min_df`：过滤极低频噪声\n",
        "- `class_weight='balanced'`：以防比例不完全平衡\n",
        "\n",
        "> 注意：baseline 不做 embedding、不用深度网络，符合 OBJ2 要求。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "tfidf_lr = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        ngram_range=(1,2),\n",
        "        min_df=2,\n",
        "        max_df=0.95,\n",
        "        sublinear_tf=True,\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        max_iter=200,\n",
        "        solver=\"saga\",\n",
        "        n_jobs=-1,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=RANDOM_STATE,\n",
        "    ))\n",
        "])\n",
        "\n",
        "X_train, y_train = train_df[\"text_clean\"], train_df[\"label\"]\n",
        "X_val, y_val     = val_df[\"text_clean\"],   val_df[\"label\"]\n",
        "X_test, y_test   = test_df[\"text_clean\"],  test_df[\"label\"]\n",
        "\n",
        "tfidf_lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"val_acc:\", tfidf_lr.score(X_val, y_val))\n",
        "print(\"test_acc:\", tfidf_lr.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 评估：Accuracy + Precision/Recall/F1 + Confusion Matrix\n",
        "\n",
        "OBJ4 会更系统，但这里先给 baseline 的核心评估结果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = tfidf_lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 保存产物（给 GUI/后续对比复用）\n",
        "\n",
        "保存：\n",
        "- `baseline_tfidf_lr.joblib`：整个 pipeline（含向量化器+模型）\n",
        "- `splits/*.csv`：三份划分（便于复现实验）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = Path(\"artifacts\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "joblib.dump(tfidf_lr, OUT_DIR / \"baseline_tfidf_lr.joblib\")\n",
        "\n",
        "SPLIT_DIR = OUT_DIR / \"splits\"\n",
        "SPLIT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "train_df.to_csv(SPLIT_DIR / \"train_60.csv\", index=False)\n",
        "val_df.to_csv(SPLIT_DIR / \"val_10.csv\", index=False)\n",
        "test_df.to_csv(SPLIT_DIR / \"test_30.csv\", index=False)\n",
        "\n",
        "print(\"Saved to:\", OUT_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 快速推理：输出用于 Happiness Index 的分数\n",
        "\n",
        "Logistic Regression 支持 `predict_proba`，可以把 `P(pos)` 当作情绪强度，再做窗口聚合（daily/weekly）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_sentiment(texts):\n",
        "    texts_clean = [preprocess(t) for t in texts]\n",
        "    proba_pos = tfidf_lr.predict_proba(texts_clean)[:,1]\n",
        "    label = (proba_pos >= 0.5).astype(int)\n",
        "    return proba_pos, label\n",
        "\n",
        "samples = [\n",
        "    \"I am sooooo happy!!!\",\n",
        "    \"not good at all...\",\n",
        "    \"This is fine? maybe.\"\n",
        "]\n",
        "\n",
        "proba, label = predict_sentiment(samples)\n",
        "list(zip(samples, proba.round(4), label))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
